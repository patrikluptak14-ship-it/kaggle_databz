# -*- coding: utf-8 -*-  # podpora pre diakritiku a UTF-8 v sÃºbore
"""
UNIVERSAL KAGGLE ANALYZER â€“ kompletne okomentovanÃ½ skript pre univerzÃ¡lnu analÃ½zu CSV z Kaggle.
Tento sÃºbor je rozdelenÃ½ do 4 ÄastÃ­ v chate. Tu je ÄŒASÅ¤ 1/4.
Po skopÃ­rovanÃ­ vÅ¡etkÃ½ch ÄastÃ­ za seba vznikne jeden sÃºvislÃ½ .py skript.

ÄŒo tento skript robÃ­:
- NaÄÃ­ta Ä¾ubovoÄ¾nÃ½ CSV sÃºbor (predvolene db1.csv alebo cez env CSV_PATH)
- UrobÃ­ EDA (head/info/describe), zÃ¡kladnÃ© Äistenie (duplicity, NaN)
- Automaticky rozpoznÃ¡ dÃ¡tumovÃ© stÄºpce a vytvorÃ­ year/month/day/hour
- PokÃºsi sa autodetekovaÅ¥ cieÄ¾ovÃ½ stÄºpec (target)
- RozlÃ­Å¡i typ Ãºlohy (klasifikÃ¡cia alebo regresia)
- PripravÃ­ nÃ¡stroje na spracovanie ÄÃ­selnÃ½ch a textovÃ½ch stÄºpcov (imputer, Å¡kÃ¡lovanie, OneHot)
- NatrÃ©nuje viacerÃ© modely (zvolenÃ© v konfigurÃ¡cii), vyberie najlepÅ¡Ã­ a uloÅ¾Ã­ ho
- UloÅ¾Ã­ vÃ½stupy: grafy, predictions_test.csv, summary.txt, cleaned_data.csv, .joblib model
"""

# ====== Importy kniÅ¾nÃ­c ======
import os  # prÃ¡ca so sÃºbormi, prieÄinkami a prostredÃ­m
import sys  # umoÅ¾nÃ­ korektne ukonÄiÅ¥ program (sys.exit) pri chybÃ¡ch
import json  # formÃ¡t JSON pre uloÅ¾enie sÃºhrnov a vÃ½sledkov
import joblib  # ukladanie a naÄÃ­tanie natrÃ©novanÃ½ch modelov (pickle-like)
import numpy as np  # rÃ½chle numerickÃ© vÃ½poÄty a polia
import pandas as pd  # prÃ¡ca s tabuÄ¾kovÃ½mi dÃ¡tami (DataFrame)
import matplotlib.pyplot as plt  # kreslenie grafov

from typing import Optional, List  # typovÃ© anotÃ¡cie pre ÄitateÄ¾nosÅ¥

# Z balÃ­ka scikit-learn importujeme stavebnice na spracovanie dÃ¡t a modely
from sklearn.compose import ColumnTransformer  # kombinuje spracovanie numerickÃ½ch/kategÃ³riÃ­
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder  # kÃ³dovanie a Å¡kÃ¡lovanie
from sklearn.pipeline import Pipeline  # pospÃ¡ja kroky (preprocessing + model) do jednÃ©ho objektu
from sklearn.impute import SimpleImputer  # dopÄºÅˆanie chÃ½bajÃºcich hodnÃ´t
from sklearn.model_selection import train_test_split, cross_val_score  # rozdelenie dÃ¡t a krÃ­Å¾ovÃ¡ validÃ¡cia

# KlasifikaÄnÃ© modely
from sklearn.linear_model import LogisticRegression  # lineÃ¡rny klasifikÃ¡tor (baseline)
from sklearn.ensemble import RandomForestClassifier  # stromovÃ¡ metÃ³da (Äasto silnÃ¡ out-of-the-box)
from sklearn.svm import SVC  # Support Vector Classifier (nelineÃ¡rny RBF)

# RegresnÃ© modely
from sklearn.linear_model import LinearRegression  # klasickÃ¡ lineÃ¡rna regresia
from sklearn.ensemble import RandomForestRegressor  # stromovÃ¡ regresia
from sklearn.svm import SVR  # Support Vector Regressor (nelineÃ¡rny RBF)

# Metriky pre klasifikÃ¡ciu a regresiu
from sklearn.metrics import (
    accuracy_score,                # presnosÅ¥ (klasifikÃ¡cia)
    f1_score,                      # F1 skÃ³re (vÃ¡Å¾enÃ© pri nerovnovÃ¡he)
    classification_report,         # precision/recall/F1 po triedach
    confusion_matrix,              # matica chÃ½b
    r2_score,                      # R^2 (regresia)
    mean_absolute_error,           # MAE (regresia)
    mean_squared_error             # MSE / RMSE (regresia)
)

# ====== KonfigurÃ¡cia skriptu ======
DATA_PATH = os.environ.get("CSV_PATH", "db1.csv")  # cesta k CSV (ak je nastavenÃ¡ env premennÃ¡ CSV_PATH, pouÅ¾ije ju)
OUTPUT_DIR = "outputs"  # prieÄinok pre uloÅ¾enie grafov, modelov a CSV vÃ½stupov
RANDOM_STATE = 42  # seed nÃ¡hodnÃ©ho generÃ¡tora pre reprodukovateÄ¾nosÅ¥

TARGET_COL: Optional[str] = None  # cieÄ¾ovÃ½ stÄºpec (ak None, skript sa ho pokÃºsi nÃ¡jsÅ¥ sÃ¡m)
TARGET_CANDIDATES = [             # beÅ¾nÃ© nÃ¡zvy, pod ktorÃ½mi sa na Kaggle vyskytuje label
    "target", "label", "class", "species", "y", "output", "category", "outcome", "type"
]
FORCE_TASK: Optional[str] = None  # mÃ´Å¾eÅ¡ vynÃºtiÅ¥ "classification" alebo "regression"; inak auto

# Modely, ktorÃ© budeme skÃºÅ¡aÅ¥ pre kaÅ¾dÃ½ typ Ãºlohy (poradie = poradie trÃ©ningu)
SELECTED_MODELS_CLASS = ["LogReg", "RandomForest", "SVC"]   # pre klasifikÃ¡ciu
SELECTED_MODELS_REGR  = ["LinReg", "RandomForestReg", "SVR"]  # pre regresiu

MAX_OHE_UNIQUES = 50  # ak mÃ¡ kategÃ³ria viac unikÃ¡tov, nebudeme ju OneHot-ovaÅ¥ (aby nevybuchla dimenzia)

# Uisti sa, Å¾e vÃ½stupnÃ½ prieÄinok existuje
os.makedirs(OUTPUT_DIR, exist_ok=True)  # ak neexistuje, vytvor; ak existuje, niÄ sa nestane

# ====== PomocnÃ© funkcie ======
def safe_print(*args, **kwargs):
    """
    BezpeÄnÃ© printovanie â€“ ak by terminÃ¡l nevedel zobraziÅ¥ niektorÃ© znaky, pouÅ¾ijeme byte vÃ½stup.
    Toto len zniÅ¾uje riziko chyby pri diakritike na rÃ´znych systÃ©moch.
    """
    try:
        print(*args, **kwargs)  # pokus o Å¡tandardnÃ½ print
    except Exception:
        msg = " ".join(str(a) for a in args)  # fallback: spoj argumenty do jednÃ©ho reÅ¥azca
        sys.stdout.buffer.write((msg + "\n").encode("utf-8", errors="ignore"))  # zapÃ­Å¡ bajty v UTF-8

def detect_datetime_and_expand(df: pd.DataFrame) -> pd.DataFrame:
    """
    VyhÄ¾adÃ¡ stÄºpce s nÃ¡zvami obsahujÃºcimi 'date', 'time', 'timestamp', 'dt',
    skÃºsi ich konvertovaÅ¥ na datetime a vytvorÃ­ novÃ© featury: year, month, day, hour.
    """
    for col in list(df.columns):  # iterujeme cez menÃ¡ stÄºpcov (list, aby sme poÄas Ãºprav neiterovali nad Å¾ivÃ½m view)
        low = str(col).lower()  # nÃ¡zov stÄºpca v malÃ½ch pÃ­smenÃ¡ch
        if any(k in low for k in ["date", "time", "timestamp", "dt"]):  # jednoduchÃ© pravidlo pre detekciu dÃ¡tumu/Äasu
            try:
                dt = pd.to_datetime(df[col], errors="coerce", infer_datetime_format=True)  # konverzia na datetime
                if dt.notna().sum() > 0:  # ak sa aspoÅˆ niektorÃ© hodnoty podarilo konvertovaÅ¥
                    df[col] = dt  # prepÃ­Å¡ pÃ´vodnÃ½ stÄºpec datetime hodnotami (NaN ostanÃº NaT)
                    df[f"{col}_year"]  = dt.dt.year   # extrahuj rok
                    df[f"{col}_month"] = dt.dt.month  # extrahuj mesiac
                    df[f"{col}_day"]   = dt.dt.day    # extrahuj deÅˆ
                    try:
                        df[f"{col}_hour"] = dt.dt.hour  # ak je Äas k dispozÃ­cii, extrahuj hodinu
                    except Exception:
                        pass  # ak hodina nie je k dispozÃ­cii (napr. len dÃ¡tum), ignoruj
            except Exception:
                pass  # pri chybe konverzie jednoducho niÄ neurob
    return df  # vrÃ¡Å¥ rozÅ¡Ã­renÃ½ DataFrame

def autodetect_target(df: pd.DataFrame) -> Optional[str]:
    """
    PokÃºsi sa nÃ¡jsÅ¥ cieÄ¾ovÃ½ stÄºpec:
    1) ak sa niektorÃ½ zo znÃ¡mych nÃ¡zvov TARGET_CANDIDATES nachÃ¡dza v stÄºpcoch, vrÃ¡ti ho
    2) ak existuje presne 1 kateg. stÄºpec (object/category), vrÃ¡ti ho
    3) inak prechÃ¡dza stÄºpce odzadu a hÄ¾adÃ¡ takÃ½ s <= 20 unikÃ¡tmi a ktorÃ½ nie je float
    4) fallback: ak poslednÃ½ stÄºpec je kateg. alebo mÃ¡ mÃ¡lo unikÃ¡tov, vrÃ¡ti poslednÃ½
    """
    for c in TARGET_CANDIDATES:  # najprv pokus podÄ¾a beÅ¾nÃ½ch nÃ¡zvov
        if c in df.columns:
            return c
    cat_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()  # kategÃ³rie
    if len(cat_cols) == 1:  # ak je len jedna kategÃ³ria, pravdepodobne label
        return cat_cols[0]
    for col in df.columns[::-1]:  # skÃºs od konca (v mnohÃ½ch CSV bÃ½va label vpravo)
        nunq = df[col].nunique(dropna=True)  # poÄet unikÃ¡tnych hodnÃ´t
        if nunq <= 20 and str(df[col].dtype) != "float64":  # mÃ¡lo unikÃ¡tov a nie je to float (Äasto je to trieda)
            return col
    last = df.columns[-1]  # fallback: poslednÃ½ stÄºpec
    nunq = df[last].nunique(dropna=True)  # poÄet unikÃ¡tov poslednÃ©ho stÄºpca
    if (df[last].dtype in ["object", "category"]) or (nunq <= 20):  # ak mÃ¡lo unikÃ¡tov alebo kategÃ³ria
        return last
    return None  # nenaÅ¡iel sa vhodnÃ½ cieÄ¾

def decide_task_type(series: pd.Series) -> str:
    """
    Rozhodnutie typu Ãºlohy:
    - ak je FORCE_TASK nastavenÃ½, vrÃ¡Å¥ ho
    - ak je stÄºpec numerickÃ½ a mÃ¡ veÄ¾a unikÃ¡tov -> REGRESIA
      inak -> KLASIFIKÃCIA
    - ak nie je numerickÃ½ -> KLASIFIKÃCIA
    """
    if FORCE_TASK in {"classification", "regression"}:  # manuÃ¡lne prebitie
        return FORCE_TASK
    if pd.api.types.is_numeric_dtype(series):  # ak je ÄÃ­slo
        return "classification" if series.nunique(dropna=True) <= 20 else "regression"
    return "classification"  # textovÃ©/kat. ciele sÃº klasifikÃ¡cia

def limit_categorical_uniques(df: pd.DataFrame, cats: List[str]) -> List[str]:
    """
    Z danej mnoÅ¾iny kategÃ³riÃ­ vrÃ¡Å¥ len tie, ktorÃ© majÃº â‰¤ MAX_OHE_UNIQUES unikÃ¡tov.
    ZabraÅˆuje to vytvoreniu obrovskÃ©ho OneHot priestoru pri high-cardinality stÄºpcoch.
    """
    return [c for c in cats if df[c].nunique(dropna=True) <= MAX_OHE_UNIQUES]

def summarize_df(df: pd.DataFrame) -> str:
    """
    Zhrnutie: poÄet riadkov/stÄºpcov, typy stÄºpcov a poÄty chÃ½bajÃºcich hodnÃ´t.
    VÃ½sledok sa uloÅ¾Ã­ do summary.txt pre prehÄ¾ad.
    """
    lines = []
    lines.append(f"Rows: {len(df)}, Columns: {len(df.columns)}")  # celkovÃ© rozmery
    lines.append("\nColumn dtypes:\n" + str(df.dtypes))  # dÃ¡tovÃ© typy
    lines.append("\nMissing values per column:\n" + str(df.isnull().sum()))  # poÄty NaN
    return "\n".join(lines)  # spoj riadky do jednÃ©ho textu

# ====== NaÄÃ­tanie CSV ======
try:
    df = pd.read_csv(DATA_PATH)  # pokus naÄÃ­taÅ¥ CSV do DataFrame
except FileNotFoundError:
    sys.exit(f"âŒ SÃºbor {DATA_PATH} neexistuje. Umiestni CSV k skriptu alebo nastav env CSV_PATH.")  # koniec s chybou

# ZÃ¡kladnÃ© EDA vÃ½pisy do konzoly (bezpeÄnÃ© printy)
safe_print("âœ… Dataset naÄÃ­tanÃ½:", DATA_PATH)  # informÃ¡cia, Å¾e CSV sa naÄÃ­talo
safe_print("\nğŸ” UkÃ¡Å¾ka dÃ¡t:\n", df.head())  # prvÃ½ch 5 riadkov pre vizuÃ¡lnu kontrolu
safe_print("\nâ„¹ï¸ info():")  # nadpis
safe_print(df.info())  # typy stÄºpcov a poÄty neprÃ¡zdnych hodnÃ´t
safe_print("\nğŸ“Š describe():\n", df.describe(include='all', datetime_is_numeric=True))  # Å¡tatistiky (aj pre datetime)

# ====== ZÃ¡kladnÃ© Äistenie a rozÅ¡Ã­renie o dÃ¡tumovÃ© featury ======
before = len(df)  # poÄet riadkov pred odstraÅˆovanÃ­m duplicit
df = df.drop_duplicates()  # odstrÃ¡Åˆ duplicitnÃ© riadky
safe_print(f"\nğŸ§¹ OdstrÃ¡nenÃ½ch duplicitnÃ½ch riadkov: {before - len(df)}")  # vypÃ­Å¡, koÄ¾ko ich bolo

df = detect_datetime_and_expand(df)  # pridaj *_year/_month/_day/_hour ak sÃº dÃ¡tumy

# ====== Autodetekcia cieÄ¾a a typu Ãºlohy ======
if TARGET_COL is None:              # ak cieÄ¾ nie je zadanÃ½ ruÄne
    TARGET_COL = autodetect_target(df)  # skÃºs ho nÃ¡jsÅ¥ automaticky

if TARGET_COL is None or TARGET_COL not in df.columns:  # ak sa cieÄ¾ nenaÅ¡iel
    task_type = None  # nevieme, Äo trÃ©novaÅ¥, budeme robiÅ¥ len EDA a uloÅ¾enie
    safe_print("\nâš ï¸ Nebol nÃ¡jdenÃ½ cieÄ¾ovÃ½ stÄºpec â€“ skript vykonÃ¡ len EDA.")
else:
    safe_print(f"\nğŸ¯ CieÄ¾ovÃ½ stÄºpec: {TARGET_COL}")  # vypÃ­Å¡, ktorÃ½ stÄºpec je cieÄ¾
    task_type = decide_task_type(df[TARGET_COL])  # urÄi typ Ãºlohy podÄ¾a cieÄ¾ovÃ©ho stÄºpca
    safe_print("ğŸ§© Typ Ãºlohy:", task_type)  # vypÃ­Å¡ typ (classification/regression)

# ====== JednoduchÃ© doplnenie NaN pre EDA a grafy ======
numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()  # zoznam ÄÃ­selnÃ½ch stÄºpcov
categorical_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()  # zoznam kategÃ³riÃ­
if TARGET_COL in numeric_cols:  # ak sa cieÄ¾ ocitol medzi numerickÃ½mi, pre EDA ho vynechÃ¡me
    numeric_cols.remove(TARGET_COL)
if TARGET_COL in categorical_cols:  # to istÃ© pre kategÃ³rie
    categorical_cols.remove(TARGET_COL)

if numeric_cols:
    # vyplÅˆ ÄÃ­selnÃ© NaN mediÃ¡nmi (jednoduchÃ© robustnÃ© rieÅ¡enie pre EDA)
    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median(numeric_only=True))
if categorical_cols:
    # kategÃ³rie vyplÅˆ textom "Unknown" (aby grafy/uloÅ¾enie nespadli)
    df[categorical_cols] = df[categorical_cols].fillna("Unknown")

# ====== RÃ½chle grafy pre EDA ======
if numeric_cols:  # ak mÃ¡me aspoÅˆ jeden ÄÃ­selnÃ½ stÄºpec
    ref = numeric_cols[0]  # zober prvÃ½ ÄÃ­selnÃ½ ako referenÄnÃ½ pre histogram
    plt.figure()  # novÃ¡ figure
    df[ref].hist()  # histogram rozdelenia hodnÃ´t
    plt.title(f"Histogram - {ref}")  # nadpis grafu
    plt.xlabel(ref); plt.ylabel("Frekvencia")  # osi
    plt.savefig(os.path.join(OUTPUT_DIR, f"hist_{ref}.png"), bbox_inches="tight")  # uloÅ¾ graf do outputs/
    plt.close()  # zavri figure, nech neprekroÄÃ­me limity

    if len(numeric_cols) >= 2:  # ak sÃº aspoÅˆ 2 ÄÃ­selnÃ©
        plt.figure()
        df.boxplot(column=list(numeric_cols[:2]))  # boxplot pre prvÃ© dva ÄÃ­selnÃ© stÄºpce
        plt.title("Boxplot â€“ prvÃ© dva ÄÃ­selnÃ© stÄºpce")
        plt.savefig(os.path.join(OUTPUT_DIR, "boxplot.png"), bbox_inches="tight")
        plt.close()

        plt.figure()
        plt.scatter(df[numeric_cols[0]], df[numeric_cols[1]])  # scatter vzÅ¥ahu prvÃ½ch dvoch ÄÃ­selnÃ½ch stÄºpcov
        plt.title(f"Scatter: {numeric_cols[0]} vs {numeric_cols[1]}")
        plt.xlabel(numeric_cols[0]); plt.ylabel(numeric_cols[1])
        plt.savefig(os.path.join(OUTPUT_DIR, "scatter.png"), bbox_inches="tight")
        plt.close()

# ====== UloÅ¾enie sÃºhrnu a oÄistenÃ½ch dÃ¡t (EDA-only fallback) ======
summary_txt = os.path.join(OUTPUT_DIR, "summary.txt")  # cesta k sÃºhrnu
with open(summary_txt, "w", encoding="utf-8") as f:  # otvor sÃºbor na zÃ¡pis (UTF-8)
    f.write("UNIVERSAL KAGGLE ANALYZER â€“ SUMMARY\n" + "="*40 + "\n\n")  # hlaviÄka
    f.write(f"Source CSV: {DATA_PATH}\n")  # zdroj
    f.write(f"Target: {TARGET_COL if TARGET_COL else 'N/A'}\n\n")  # cieÄ¾ovÃ½ stÄºpec
    f.write("Data overview:\n")  # nadpis
    f.write(summarize_df(df))  # vloÅ¾ prehÄ¾ad o dÃ¡tach (typy, missingy)

cleaned_csv = os.path.join(OUTPUT_DIR, "cleaned_data.csv")  # cesta pre oÄistenÃ© dÃ¡ta
df.to_csv(cleaned_csv, index=False)  # uloÅ¾ DataFrame do CSV bez indexu

if not task_type:  # ak sme nenaÅ¡li cieÄ¾ a teda netrÃ©nujeme
    safe_print(f"\nğŸ’¾ OÄistenÃ© dÃ¡ta uloÅ¾enÃ© do: {cleaned_csv}\nğŸ“ SÃºhrn uloÅ¾enÃ½ do: {summary_txt}")
    sys.exit(0)  # ukonÄi program â€“ EDA je hotovÃ©, ale ML ÄasÅ¥ sa nespustÃ­
# ====== PrÃ­prava dÃ¡t pre model (ak mÃ¡me TARGET a typ Ãºlohy) ======
y = df[TARGET_COL]                 # y = cieÄ¾ovÃ¡ premennÃ¡ (label / target)
X = df.drop(columns=[TARGET_COL])  # X = vÅ¡etky ostatnÃ© stÄºpce (features)

# Identifikuj numerickÃ© a kategÃ³rie po odobratÃ­ targetu
num_cols_all = X.select_dtypes(include=["number"]).columns.tolist()            # vÅ¡etky ÄÃ­selnÃ© vstupy
cat_cols_all = X.select_dtypes(include=["object", "category"]).columns.tolist()  # vÅ¡etky kategÃ³rie vstupy

# Obmedz kategÃ³rie s prÃ­liÅ¡ veÄ¾a unikÃ¡tmi, aby OneHot neexplodoval
cat_cols_ohe = [c for c in cat_cols_all if X[c].nunique(dropna=True) <= MAX_OHE_UNIQUES]  # ponechaj len rozumnÃ©

# Vytvor pre-processing pipelines pre ÄÃ­sla a kategÃ³rie
numeric_transform = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),   # doplÅˆ NaN mediÃ¡nmi
    ("scaler", StandardScaler())                     # Å¡kÃ¡luj na mean=0, std=1 (vhodnÃ© pre LR/SVC/SVR)
])

categorical_transform = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),  # doplÅˆ NaN najÄastejÅ¡ou hodnotou
    ("onehot", OneHotEncoder(handle_unknown="ignore"))     # zakÃ³duj kategÃ³rie; ignoruj neznÃ¡me pri teste
])

# Poskladaj ColumnTransformer: ÄÃ­sla idÃº cez numeric_transform, kategÃ³rie cez categorical_transform
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transform, num_cols_all),   # transformuj numerickÃ© stÄºpce
        ("cat", categorical_transform, cat_cols_ohe)  # transformuj kategÃ³rie s rozumnou kardinalitou
    ],
    remainder="drop"  # ostatnÃ© stÄºpce (napr. veÄ¾mi high-cardinality) zahoÄ
)

# ====== Rozhodnutie vetvy: klasifikÃ¡cia vs. regresia ======
if task_type == "classification":  # ak ide o klasifikÃ¡ciu
    # Label-encode y, ak je textovÃ½ alebo kategÃ³ria (prÃ­p. mÃ¡ priveÄ¾a unikÃ¡tov a nie je int)
    if y.dtype in ["object", "category"] or (y.nunique() > 20 and not pd.api.types.is_integer_dtype(y)):
        le = LabelEncoder()                     # vytvor encoder
        y_enc = le.fit_transform(y.astype(str)) # prevedie triedy na ÄÃ­sla
        class_names = list(le.classes_)         # uloÅ¾ si mapovanie spÃ¤Å¥ na textovÃ© menÃ¡
    else:
        le = None                               # netreba kÃ³dovaÅ¥
        y_enc = y.to_numpy()                    # rovno pouÅ¾ijeme numerickÃ½ target
        class_names = sorted(list(pd.Series(y_enc).unique()))  # menÃ¡ tried sÃº ÄÃ­selnÃ©

    # StratifikovanÃ½ split (ak mÃ¡me aspoÅˆ 2 triedy), inak bez stratifikÃ¡cie
    strat = y_enc if pd.Series(y_enc).nunique() > 1 else None  # stratifikuj, ak to dÃ¡va zmysel
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_enc, test_size=0.25, random_state=RANDOM_STATE, stratify=strat
    )

    # Definuj zoo klasifikaÄnÃ½ch modelov â€“ kaÅ¾dÃ½ v pipeline s preprocessorom
    model_zoo = {
        "LogReg": Pipeline([
            ("prep", preprocessor),                                    # preprocessing
            ("clf", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))  # klasifikÃ¡tor
        ]),
        "RandomForest": Pipeline([
            ("prep", preprocessor),
            ("clf", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE))
        ]),
        "SVC": Pipeline([
            ("prep", preprocessor),
            ("clf", SVC(kernel="rbf", probability=True, random_state=RANDOM_STATE))
        ])
    }

    # Vyber len tie, ktorÃ© si si nadefinoval v SELECTED_MODELS_CLASS
    selected = [m for m in SELECTED_MODELS_CLASS if m in model_zoo]  # poradie zachovanÃ©

    results = []             # sem budeme ukladaÅ¥ vÃ½sledky modelov (accuracy, F1, CV)
    best_name = None         # meno najlepÅ¡ieho modelu
    best_model = None        # samotnÃ½ natrÃ©novanÃ½ objekt pipeline
    best_acc = -1.0          # najlepÅ¡ia accuracy (zaÄneme veÄ¾mi nÃ­zko)

    # TrÃ©nuj a vyhodnoÅ¥ kaÅ¾dÃ½ vybranÃ½ model
    for name in selected:
        pipe = model_zoo[name]                         # zober pipeline
        pipe.fit(X_train, y_train)                     # natrÃ©nuj ju na trÃ©ningovÃ½ch dÃ¡tach
        y_pred = pipe.predict(X_test)                  # predikcie na testovacÃ­ch dÃ¡tach

        acc = accuracy_score(y_test, y_pred)          # presnosÅ¥
        f1w = f1_score(y_test, y_pred, average="weighted", zero_division=0)  # F1 vÃ¡Å¾enÃ©

        # KrÃ­Å¾ovÃ¡ validÃ¡cia pre orientaÄnÃ½ obraz o stabilite
        try:
            cv_scores = cross_val_score(pipe, X, y_enc, cv=5, scoring="accuracy")  # 5-fold CV
            cv_line = f"CV acc: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}"     # sprÃ¡va
        except Exception as e:
            cv_line = f"CV error: {e}"  # ak nieÄo padne (napr. prÃ­liÅ¡ mÃ¡lo vzoriek), zapÃ­Å¡ chybu

        # VÃ½stupy do konzoly
        safe_print(f"\nâ€” {name} â€”\nAccuracy: {acc:.4f}\nF1-weighted: {f1w:.4f}\n{cv_line}\n")
        safe_print(
            "Classification report:\n",
            classification_report(
                y_test, y_pred, zero_division=0, target_names=[str(c) for c in class_names]
            )
        )

        # UloÅ¾ aj confusion matrix ako obrÃ¡zok
        cm = confusion_matrix(y_test, y_pred)  # matica chÃ½b
        plt.figure()
        plt.imshow(cm, interpolation='nearest')  # heatmap-like vizualizÃ¡cia
        plt.title(f"Confusion matrix - {name}")
        plt.xlabel("Predicted"); plt.ylabel("True")
        plt.savefig(os.path.join(OUTPUT_DIR, f"cm_{name}.png"), bbox_inches="tight")
        plt.close()

        # Ak je aktuÃ¡lny model lepÅ¡Ã­ podÄ¾a accuracy, prepis najlepÅ¡ieho
        if acc > best_acc:
            best_acc = acc
            best_name = name
            best_model = pipe

        # UloÅ¾ si vÃ½sledky do prehÄ¾adu (budÃº zapÃ­sanÃ© aj do summary.txt)
        results.append({
            "model": name,
            "accuracy": float(acc),
            "f1_weighted": float(f1w),
            "cv": cv_line
        })

    # ====== UloÅ¾enie artefaktov pre klasifikÃ¡ciu ======
    # Funkcia na spÃ¤tnÃ© dekÃ³dovanie ÄÃ­selnÃ½ch Å¡tÃ­tkov na textovÃ© (ak sme LabelEncoder pouÅ¾ili)
    def inv_label(vals):
        return le.inverse_transform(vals) if le is not None else vals  # ak le neexistuje, vrÃ¡Å¥ pÃ´vodnÃ© ÄÃ­sla

    # MalÃ½ nÃ¡hÄ¾ad predikciÃ­ (prvÃ½ch 5 z testu)
    preview_n = min(5, len(y_test))  # vezmi max 5 vzoriek, alebo menej ak test je menÅ¡Ã­
    preview = pd.DataFrame({
        "true": inv_label(y_test[:preview_n]),                  # skutoÄnÃ© triedy
        "pred": inv_label(best_model.predict(X_test[:preview_n]))  # predikovanÃ© triedy
    })
    preview_path = os.path.join(OUTPUT_DIR, "preview_predictions.csv")  # kam uloÅ¾Ã­me nÃ¡hÄ¾ad
    preview.to_csv(preview_path, index=False)  # uloÅ¾enie CSV s nÃ¡hÄ¾adom

    # UloÅ¾ aj plnÃ© predikcie na celÃ½ test
    full_pred = pd.DataFrame({
        "y_true": inv_label(y_test),
        "y_pred": inv_label(best_model.predict(X_test))
    })
    full_pred_path = os.path.join(OUTPUT_DIR, "predictions_test.csv")  # cesta
    full_pred.to_csv(full_pred_path, index=False)  # uloÅ¾ plnÃ© predikcie

    # UloÅ¾ najlepÅ¡Ã­ model (vrÃ¡tane preprocesingu v pipeline)
    model_path = os.path.join(OUTPUT_DIR, f"best_model_{best_name}.joblib")  # nÃ¡zov sÃºboru podÄ¾a modelu
    joblib.dump(best_model, model_path)  # dump modelu

    # DopiÅ¡ sÃºhrn do summary.txt (append)
    with open(summary_txt, "a", encoding="utf-8") as f:
        f.write("\n\n=== CLASSIFICATION RESULTS ===\n")  # nadpis
        f.write(json.dumps(results, indent=2, ensure_ascii=False))  # vÃ½sledky v JSON formÃ¡te
        f.write(f"\n\nBest model: {best_name}\nSaved model: {model_path}\n")  # info o najlepÅ¡om modeli
        f.write(f"Preview preds: {preview_path}\nFull preds: {full_pred_path}\n")  # kde nÃ¡jsÅ¥ predikcie

    # VÃ½pis dÃ´leÅ¾itÃ½ch ciest do konzoly
    safe_print(f"\nğŸ† Best model: {best_name} (saved: {model_path})")
    safe_print(f"ğŸ”® Preview predictions -> {preview_path}")
    safe_print(f"ğŸ’¾ Full predictions -> {full_pred_path}")

# ====== Vetva pre REGRESIU ======
else:
    # Ak je Ãºloha regresia, split bez stratifikÃ¡cie (y je spojitÃ½)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=RANDOM_STATE
    )

    # Definuj modely pre regresiu
    model_zoo = {
        "LinReg": Pipeline([
            ("prep", preprocessor),
            ("reg", LinearRegression())
        ]),
        "RandomForestReg": Pipeline([
            ("prep", preprocessor),
            ("reg", RandomForestRegressor(n_estimators=400, random_state=RANDOM_STATE))
        ]),
        "SVR": Pipeline([
            ("prep", preprocessor),
            ("reg", SVR(kernel="rbf"))
        ])
    }

    # PodÄ¾a konfigurÃ¡cie vyber ktorÃ© trÃ©novaÅ¥
    selected = [m for m in SELECTED_MODELS_REGR if m in model_zoo]

    results = []           # vÃ½sledky modelov (R2, MAE, RMSE, CV)
    best_name = None       # najlepÅ¡Ã­ nÃ¡zov
    best_model = None      # najlepÅ¡Ã­ pipeline objekt
    best_r2 = -1e9         # najlepÅ¡Ã­ R^2 (zaÄÃ­name veÄ¾mi nÃ­zko)

    # TrÃ©nuj a vyhodnocuj kaÅ¾dÃ½ model
    for name in selected:
        pipe = model_zoo[name]          # pipeline s preprocesingom
        pipe.fit(X_train, y_train)      # trÃ©ning na trÃ©ningovej mnoÅ¾ine
        y_pred = pipe.predict(X_test)   # predikcia na testovacej mnoÅ¾ine

        # Metriky regresie
        r2  = r2_score(y_test, y_pred)                          # R^2 (vyÅ¡Å¡ie = lepÅ¡ie)
        mae = mean_absolute_error(y_test, y_pred)               # MAE (niÅ¾Å¡ie = lepÅ¡ie)
        rmse = mean_squared_error(y_test, y_pred, squared=False)  # RMSE (niÅ¾Å¡ie = lepÅ¡ie)

        # KrÃ­Å¾ovÃ¡ validÃ¡cia (R2)
        try:
            cv_scores = cross_val_score(pipe, X, y, cv=5, scoring="r2")  # 5-fold CV pre R2
            cv_line = f"CV R2: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}"
        except Exception as e:
            cv_line = f"CV error: {e}"

        # VÃ½pis do konzoly
        safe_print(f"\nâ€” {name} â€”\nR2: {r2:.4f}\nMAE: {mae:.4f}\nRMSE: {rmse:.4f}\n{cv_line}\n")

        # Sleduj najlepÅ¡Ã­ podÄ¾a R2
        if r2 > best_r2:
            best_r2 = r2
            best_name = name
            best_model = pipe

        # UloÅ¾ vÃ½sledky do zoznamu
        results.append({
            "model": name,
            "r2": float(r2),
            "mae": float(mae),
            "rmse": float(rmse),
            "cv": cv_line
        })

    # UloÅ¾enie najlepÅ¡ieho modelu
    model_path = os.path.join(OUTPUT_DIR, f"best_model_{best_name}.joblib")
    joblib.dump(best_model, model_path)

    # UloÅ¾ predikcie na test
    full_pred = pd.DataFrame({"y_true": y_test, "y_pred": best_model.predict(X_test)})
    full_pred_path = os.path.join(OUTPUT_DIR, "predictions_test.csv")
    full_pred.to_csv(full_pred_path, index=False)

    # DopiÅ¡ do summary.txt (append)
    with open(summary_txt, "a", encoding="utf-8") as f:
        f.write("\n\n=== REGRESSION RESULTS ===\n")
        f.write(json.dumps(results, indent=2, ensure_ascii=False))
        f.write(f"\n\nBest model: {best_name}\nSaved model: {model_path}\n")
        f.write(f"Predictions: {full_pred_path}\n")

    # VÃ½pis ciest do konzoly
    safe_print(f"\nğŸ† Best model: {best_name} (saved: {model_path})")
    safe_print(f"ğŸ’¾ Predictions -> {full_pred_path}")

# ====== UloÅ¾enie oÄistenÃ½ch dÃ¡t a summary (zÃ¡vereÄnÃ½ krok) ======
# Pozn.: cleaned_data.csv sme uÅ¾ uloÅ¾ili skÃ´r (pre EDA-only vetvu aj pre ML vetvu),
# ale znovu uloÅ¾enie je idempotentnÃ© â€“ nevadÃ­, ak sa prepÃ­Å¡e novÅ¡ou verziou.

df.to_csv(cleaned_csv, index=False)  # uloÅ¾ DataFrame po zÃ¡kladnÃ½ch EDA ÃºpravÃ¡ch
safe_print(f"\nğŸ’¾ Cleaned -> {cleaned_csv}")  # informÃ¡cia o ceste k oÄistenÃ½m dÃ¡tam
safe_print(f"ğŸ“ Summary -> {summary_txt}")    # informÃ¡cia o ceste k sÃºhrnu

# ====== FINÃLNA SPRÃVA ======
safe_print("\nâœ… DONE.")  # jasnÃ© ukonÄenie skriptu pre pouÅ¾Ã­vateÄ¾a

# ====== PoznÃ¡mky k pouÅ¾Ã­vaniu ======
# 1) CSV si daj k tomuto skriptu (alebo nastav env premennÃº CSV_PATH).
# 2) InÅ¡talÃ¡cia zÃ¡vislostÃ­:
#      pip install pandas numpy matplotlib scikit-learn joblib
# 3) Spustenie:
#      python universal_kaggle_analysis_commented.py
# 4) VÃ½stupy hÄ¾adaj v prieÄinku outputs/ :
#      - cleaned_data.csv        (oÄistenÃ© dÃ¡ta)
#      - summary.txt             (prehÄ¾ad a metriky)
#      - cm_*.png                (confusion matrices pri klasifikÃ¡cii)
#      - hist_*.png, boxplot.png, scatter.png (zÃ¡kladnÃ© grafy)
#      - predictions_test.csv    (predikcie na testovacej mnoÅ¾ine)
#      - best_model_*.joblib     (najlepÅ¡Ã­ model s preprocesingom)
#
# 5) Ãšpravy:
#    - Ak chceÅ¡ vynÃºtiÅ¥ typ Ãºlohy, nastav FORCE_TASK = "classification" alebo "regression".
#    - Ak poznÃ¡Å¡ cieÄ¾ovÃ½ stÄºpec, nastav TARGET_COL = "tvoj_label".
#    - Ak chceÅ¡ pridaÅ¥/odobraÅ¥ modely, uprav SELECTED_MODELS_CLASS / SELECTED_MODELS_REGR.
#    - MAX_OHE_UNIQUES uprav podÄ¾a potreby, ak mÃ¡Å¡ veÄ¾a kategÃ³riÃ­ (napr. mestÃ¡, nÃ¡zvy produktov).
#
# 6) BezpeÄnostnÃ© poznÃ¡mky:
#    - Tento skript je urÄenÃ½ na rÃ½chlu explorÃ¡ciu; pre produkciu zvaÅ¾ validÃ¡ciu vstupov,
#      lepÅ¡Ã­ tuning modelov (GridSearchCV/RandomizedSearchCV), pipeline persistenciu verziÃ­, atÄ.
#
# 7) RozÅ¡Ã­renia (nÃ¡pady):
#    - PridaÅ¥ automatickÃ© ukladanie feature importances pri RandomForeste (klasifikÃ¡cia aj regresia).
#    - Export confusion matrix aj ako textovÃ¡ tabuÄ¾ka (nie len obrÃ¡zok).
#    - AutomatickÃ¡ detekcia extrÃ©mov/outlierov a report.
#    - AutomatickÃ½ profilovacÃ­ report (pandas-profiling/ydata-profiling), ak je povolenÃ©.
#
# 8) NajÄastejÅ¡ie problÃ©my:
#    - CSV sa nenaÄÃ­ta: skontroluj cestu (DATA_PATH alebo env CSV_PATH).
#    - Å½iadny target: nastav TARGET_COL ruÄne alebo uprav autodetekciu.
#    - PriveÄ¾a kategÃ³riÃ­: znÃ­Å¾ MAX_OHE_UNIQUES alebo urob target encoding (pokroÄilÃ©).
#    - MalÃ½ dataset: CV mÃ´Å¾e zlyhaÅ¥; skript chybu zaloguje a pokraÄuje.
